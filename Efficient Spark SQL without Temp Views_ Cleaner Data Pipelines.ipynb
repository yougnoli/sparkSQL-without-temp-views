{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed03136-b1d0-4a2f-9be0-dc52a8279338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer - Customer data:\n+---+------+-------+------+\n| id|  nome|cognome| citta|\n+---+------+-------+------+\n|  1| Mario|  Rossi|  Roma|\n|  2|  Luca|Bianchi|Milano|\n|  3|Giulia|  Verdi|Napoli|\n|  4|  Anna|   Neri|Torino|\n+---+------+-------+------+\n\nBronze layer - Orders data:\n+-----------+--------+------+\n|customer_id|order_id|amount|\n+-----------+--------+------+\n|          1|     101| 250.5|\n|          1|     102|  80.0|\n|          2|     103| 300.0|\n|          3|     104| 150.0|\n|          3|     105|  20.0|\n|          4|     106| 500.0|\n|          4|     107|  45.0|\n+-----------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Fake data creation (bronze layer)\n",
    "# Customers data: id, first name, last name, and city\n",
    "customer_data = [\n",
    "    (1, \"Mario\",  \"Rossi\",   \"Roma\"),\n",
    "    (2, \"Luca\",   \"Bianchi\", \"Milano\"),\n",
    "    (3, \"Giulia\", \"Verdi\",   \"Napoli\"),\n",
    "    (4, \"Anna\",   \"Neri\",    \"Torino\")\n",
    "]\n",
    "\n",
    "# Orders data: customer_id, order_id, amount\n",
    "orders_data = [\n",
    "    (1, 101, 250.5),\n",
    "    (1, 102, 80.0),\n",
    "    (2, 103, 300.0),\n",
    "    (3, 104, 150.0),\n",
    "    (3, 105, 20.0),\n",
    "    (4, 106, 500.0),\n",
    "    (4, 107, 45.0)\n",
    "]\n",
    "\n",
    "# Define schemas for DataFrame creation\n",
    "customer_schema = \"id INT, nome STRING, cognome STRING, citta STRING\"\n",
    "orders_schema = \"customer_id INT, order_id INT, amount DOUBLE\"\n",
    "\n",
    "# Create DataFrames (bronze)\n",
    "df_customers = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "df_orders = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "# Show raw data\n",
    "print(\"Bronze layer - Customer data:\")\n",
    "df_customers.show()\n",
    "\n",
    "print(\"Bronze layer - Orders data:\")\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced66ffb-cb22-408a-bbf2-92fdc227fd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query From Temp Views:\n+-----------+------+-------+------+--------+------+\n|customer_id|  nome|cognome| citta|order_id|amount|\n+-----------+------+-------+------+--------+------+\n|          1| Mario|  Rossi|  Roma|     101| 250.5|\n|          2|  Luca|Bianchi|Milano|     103| 300.0|\n|          3|Giulia|  Verdi|Napoli|     104| 150.0|\n|          4|  Anna|   Neri|Torino|     106| 500.0|\n+-----------+------+-------+------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_customers.createOrReplaceTempView(\"customers_view\")\n",
    "df_orders.createOrReplaceTempView(\"orders_view\")\n",
    "\n",
    "silver_df_from_temp = spark.sql(\"\"\"\n",
    "    SELECT c.id AS customer_id, c.nome, c.cognome, c.citta, o.order_id, o.amount\n",
    "    FROM customers_view c\n",
    "    INNER JOIN orders_view o\n",
    "        ON c.id = o.customer_id\n",
    "    WHERE o.amount > 100\n",
    "\"\"\")\n",
    "\n",
    "print(\"Query From Temp Views:\")\n",
    "silver_df_from_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546ea2e8-8425-4091-925f-9af25d15fed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver layer - Joined and filtered data (orders with amount > 100):\n+-----------+------+-------+------+--------+------+\n|customer_id|  nome|cognome| citta|order_id|amount|\n+-----------+------+-------+------+--------+------+\n|          1| Mario|  Rossi|  Roma|     101| 250.5|\n|          2|  Luca|Bianchi|Milano|     103| 300.0|\n|          3|Giulia|  Verdi|Napoli|     104| 150.0|\n|          4|  Anna|   Neri|Torino|     106| 500.0|\n+-----------+------+-------+------+--------+------+\n\nSilver layer - Customer-level aggregation:\n+-----------+------+-------+------+------------+------------+\n|customer_id|  nome|cognome| citta|total_amount|orders_count|\n+-----------+------+-------+------+------------+------------+\n|          1| Mario|  Rossi|  Roma|       250.5|           1|\n|          2|  Luca|Bianchi|Milano|       300.0|           1|\n|          3|Giulia|  Verdi|Napoli|       150.0|           1|\n|          4|  Anna|   Neri|Torino|       500.0|           1|\n+-----------+------+-------+------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Transformation into silver layer\n",
    "silver_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        c.id as customer_id,\n",
    "        c.nome,\n",
    "        c.cognome,\n",
    "        c.citta,\n",
    "        o.order_id,\n",
    "        o.amount\n",
    "    FROM {df_customers} c\n",
    "    INNER JOIN {df_orders} o\n",
    "        ON c.id = o.customer_id\n",
    "    WHERE o.amount > 100\n",
    "    \"\"\",\n",
    "    df_customers=df_customers,\n",
    "    df_orders=df_orders\n",
    ")\n",
    "\n",
    "print(\"Silver layer - Joined and filtered data (orders with amount > 100):\")\n",
    "silver_df.show()\n",
    "\n",
    "# Further aggregation in the silver layer\n",
    "silver_agg_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        nome,\n",
    "        cognome,\n",
    "        citta,\n",
    "        SUM(amount) as total_amount,\n",
    "        COUNT(order_id) as orders_count\n",
    "    FROM {silver_df}\n",
    "    GROUP BY customer_id, nome, cognome, citta\n",
    "    \"\"\",\n",
    "    silver_df=silver_df\n",
    ")\n",
    "\n",
    "print(\"Silver layer - Customer-level aggregation:\")\n",
    "silver_agg_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Efficient Spark SQL without Temp Views: Cleaner Data Pipelines",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}